
@article{DEL07,
	author={F. Dehne and T. Eavis and B. Liang},
	title={Compressing Data Cubes in Parallel {OLAP} Systems},
	abstract={},
	journal={Data Science Journal},
	volume={6},
	number={},
	year={2007},
	pages={S184-S197}
} ---------------------------------
 
@article{PD07,
	author={S. Pitre and F. Dehne and A. Chan and J. Cheetham and A. Duong and A. Emili and M. Gebbia and J. Greenblatt and M. Jessulat and N. Krogan and X. Luo and A. Golshani},
	title={{PIPE}: a protein-protein interaction prediction engine based on the re-occurring short polypeptide sequences between known interacting protein pairs},
	abstract={Paper classified as "Highly accessed". 1989 downloads in the first five months. Journal impact factor: 4.96.},
	journal={BMC Bioinformatics},
	volume={7},
	number={},
	year={2006, available via PubMed at http://www.biomedcentral.com/pubmed/16872538},
	pages={365 (15 pages)}
} ---------------------------------

@article{DER07,
	author={F. Dehne and T. Eavis and A. Rau-Chaplin},
	title={The {cgmCUBE} Project: Optimizing Parallel Data Cube Generation For {ROLAP}},
	abstract={},
	journal={Distributed and Parallel Databases},
	volume={19},
	number={1},
	year={2006},
	pages={29-62}
} --------------------------------- 

@inproceedings{LDR07,
	author={M. Lawrence and F. Dehne and A. Rau-Chaplin},
	title={Implementing {OLAP} Query Fragment Aggregation and Recombination for the {OLAP} Enabled Grid},
	abstract={http://www.cs.unb.ca/profs/aubanel/hpgc/},
	booktitle={Proc. International Parallel and Distributed Processing Symposium (IPDPS), High-Performance Grid Computing Workshop},
	year={2007},
	publisher={IEEE Comp. Soc. Dig. Library},
	pages={1-8}
} ---------------------------------

@inproceedings{DLX06,
	author={F. Dehne and M. Langston and X. Luo and S. Pitre and P. Shaw and Y. Zhang},
	title={The Cluster Editing Problem: Implementations and Experiments},
	abstract={Zuerich, Switzerland, 2006},
	booktitle={Proc. Int. Workshop on Parameterized and Exact Computation (IWPEC)},
	year={2006},
	publisher={Springer LNCS 4169},
	pages={13-24}
} ---------------------------------


@inproceedings{CDE06,
	author={Y. Chen and F. Dehne and T. Eavis and D. Green and A. Rau-Chaplin and E. Sithirasenan},
	title={{cgmOLAP}: Efficient Parallel Generation and Querying of Terabyte Size {ROLAP} Data Cubes},
	abstract={Atlanta, GA, 2006},
	booktitle={Proc. 22nd Int. Conf. on Data Engineering (ICDE)},
	year={2006},
	publisher={IEEE Comp. Soc. Dig. Library},
	pages={164-164}
} ---------------------------------

@inproceedings{DFL06,
	author={F. Dehne and M. Fellows and M. Langston and F. Rosamond and K. Stevens},
	title={An $O(2^{O(k)} n^3 )$ {FPT} algorithm for the undirected feedback vertex set problem},
	abstract={Kunming, China},
	booktitle={Proc. 11th Int. Computing and Combinatorics Conf. (COCOON)},
	year={2005},
	publisher={Springer LNCS 3595},
	pages={859-869}
} ---------------------------------


@inproceedings{goodfellow2014generative,
  title={Generative adversarial nets},
  author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  booktitle={Advances in neural information processing systems},
  pages={2672--2680},
  year={2014}
}

@article{mescheder2018training,
  title={Which training methods for GANs do actually converge?},
  author={Mescheder, Lars and Geiger, Andreas and Nowozin, Sebastian},
  journal={arXiv preprint arXiv:1801.04406},
  year={2018}
}

@article{Li2014,
abstract = {We propose a parameter server framework for distributed machine learning problems. Both data and workloads are distributed over worker nodes, while the server nodes maintain globally shared parameters, represented as dense or sparse vectors and matrices. The framework manages asynchronous data communication between nodes, and supports flexible consistency models, elastic scalability, and continuous fault tolerance. To demonstrate the scalability of the proposed frame-work, we show experimental results on petabytes of real data with billions of examples and parameters on prob-lems ranging from Sparse Logistic Regression to Latent Dirichlet Allocation and Distributed Sketching.},
author = {Li, Mu and Andersen, David G and Park, Jun Woo and Ahmed, Amr and Josifovski, Vanja and Long, James and Shekita, Eugene J and Su, Bor-yiing and Li, Mu and Andersen, David G and Park, Jun Woo and Smola, Alexander J and Ahmed, Amr},
file = {:C$\backslash$:/Users/omid/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Li et al. - 2014 - Scaling Distributed Machine Learning with the Parameter Server.pdf:pdf},
isbn = {9781931971164},
journal = {Proceedings of OSDI},
mendeley-groups = {Parallel},
pages = {583--598},
title = {{Scaling Distributed Machine Learning with the Parameter Server}},
year = {2014}
}

@article{cano2016towards,
  title={Towards geo-distributed machine learning},
  author={Cano, Ignacio and Weimer, Markus and Mahajan, Dhruv and Curino, Carlo and Fumarola, Giovanni Matteo},
  journal={arXiv preprint arXiv:1603.09035},
  year={2016}
}

@article{kearns1998efficient,
  title={Efficient noise-tolerant learning from statistical queries},
  author={Kearns, Michael},
  journal={Journal of the ACM (JACM)},
  volume={45},
  number={6},
  pages={983--1006},
  year={1998},
  publisher={ACM}
}

@inproceedings{hsieh2017gaia,
  title={Gaia: Geo-Distributed Machine Learning Approaching $\{$LAN$\}$ Speeds},
  author={Hsieh, Kevin and Harlap, Aaron and Vijaykumar, Nandita and Konomis, Dimitris and Ganger, Gregory R and Gibbons, Phillip B and Mutlu, Onur},
  booktitle={14th $\{$USENIX$\}$ Symposium on Networked Systems Design and Implementation ($\{$NSDI$\}$ 17)},
  pages={629--647},
  year={2017}
}

@article{BrendanMcMahan2017,
abstract = {Modern mobile devices have access to a wealth of data suitable for learning models, which in turn can greatly improve the user experience on the device. For example, language models can improve speech recognition and text entry, and image models can automatically select good photos. However, this rich data is often privacy sensitive, large in quantity, or both, which may preclude logging to the data center and training there using conventional approaches. We advocate an alternative that leaves the training data distributed on the mobile devices, and learns a shared model by aggregating locally-computed updates. We term this decentralized approach Federated Learning. We present a practical method for the federated learning of deep networks based on iterative model averaging, and conduct an extensive empirical evaluation, considering five different model architectures and four datasets. These experiments demonstrate the approach is robust to the unbalanced and non-IID data distributions that are a defining characteristic of this setting. Communication costs are the principal constraint, and we show a reduction in required communication rounds by 10-100x as compared to synchronized stochastic gradient descent.},
archivePrefix = {arXiv},
arxivId = {arXiv:1602.05629v3},
author = {{Brendan McMahan}, H. and Moore, Eider and Ramage, Daniel and Hampson, Seth and {Ag{\"{u}}era y Arcas}, Blaise},
eprint = {arXiv:1602.05629v3},
file = {:C$\backslash$:/Users/user/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Brendan McMahan et al. - 2017 - Communication-efficient learning of deep networks from decentralized data.pdf:pdf},
journal = {Proceedings of the 20th International Conference on Artificial Intelligence and Statistics, AISTATS 2017},
mendeley-groups = {Parallel},
title = {{Communication-efficient learning of deep networks from decentralized data}},
volume = {54},
year = {2017}
}


@article{chen2016revisiting,
  title={Revisiting distributed synchronous SGD},
  author={Chen, Jianmin and Pan, Xinghao and Monga, Rajat and Bengio, Samy and Jozefowicz, Rafal},
  journal={arXiv preprint arXiv:1604.00981},
  year={2016}
}

@article{goodfellow2014qualitatively,
  title={Qualitatively characterizing neural network optimization problems},
  author={Goodfellow, Ian J and Vinyals, Oriol and Saxe, Andrew M},
  journal={arXiv preprint arXiv:1412.6544},
  year={2014}
}

@article{Hardy2019,
abstract = {A recent technical breakthrough in the domain of machine learning is the discovery and the multiple applications of Generative Adversarial Networks (GANs). Those generative models are computationally demanding, as a GAN is composed of two deep neural networks, and because it trains on large datasets. A GAN is generally trained on a single server. In this paper, we address the problem of distributing GANs so that they are able to train over datasets that are spread on multiple workers. MD-GAN is exposed as the first solution for this problem: we propose a novel learning procedure for GANs so that they fit this distributed setup. We then compare the performance of MD-GAN to an adapted version of Federated Learning to GANs, using the MNIST and CIFAR10 datasets. MD-GAN exhibits a reduction by a factor of two of the learning complexity on each worker node, while providing better performances than federated learning on both datasets. We finally discuss the practical implications of distributing GANs.},
author = {Hardy, Corentin and {Le Merrer}, Erwan and Sericola, Bruno},
doi = {10.1109/ipdps.2019.00095},
file = {:C$\backslash$:/Users/user/Downloads/08821025.pdf:pdf},
journal = {2019 IEEE International Parallel and Distributed Processing Symposium (IPDPS)},
mendeley-groups = {Parallel},
number = {ii},
pages = {866--877},
publisher = {IEEE},
title = {{MD-GAN: Multi-Discriminator Generative Adversarial Networks for Distributed Datasets}},
year = {2019}
}

@inproceedings{vallecorsa2018distributed,
  title={Distributed Training of Generative Adversarial Networks for Fast Detector Simulation},
  author={Vallecorsa, Sofia and Carminati, Federico and Khattak, Gulrukh and Podareanu, Damian and Codreanu, Valeriu and Saletore, Vikram and Pabst, Hans},
  booktitle={International Conference on High Performance Computing},
  pages={487--503},
  year={2018},
  organization={Springer}
}

@article{horovod,
  author    = {Alexander Sergeev and
               Mike Del Balso},
  title     = {Horovod: fast and easy distributed deep learning in TensorFlow},
  journal   = {CoRR},
  volume    = {abs/1802.05799},
  year      = {2018},
  url       = {http://arxiv.org/abs/1802.05799},
  archivePrefix = {arXiv},
  eprint    = {1802.05799},
  timestamp = {Mon, 13 Aug 2018 16:46:12 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1802-05799},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{dean2012large,
  title={Large scale distributed deep networks},
  author={Dean, Jeffrey and Corrado, Greg and Monga, Rajat and Chen, Kai and Devin, Matthieu and Mao, Mark and Ranzato, Marc'aurelio and Senior, Andrew and Tucker, Paul and Yang, Ke and others},
  booktitle={Advances in neural information processing systems},
  pages={1223--1231},
  year={2012}
}

@inproceedings{wen2017terngrad,
  title={Terngrad: Ternary gradients to reduce communication in distributed deep learning},
  author={Wen, Wei and Xu, Cong and Yan, Feng and Wu, Chunpeng and Wang, Yandan and Chen, Yiran and Li, Hai},
  booktitle={Advances in neural information processing systems},
  pages={1509--1519},
  year={2017}
}

\ignorecitefornumbering{}
