<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head><title>Prof. Frank Dehne</title>


  

  
  
  <meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1"></head><body style="color: rgb(0, 0, 0); background-color: rgb(255, 255, 255);" alink="#000000" link="#000000" vlink="#000000">
<table border="0" width="100%">

  <tbody>
    <tr>
      <td height="27" width="45%">
      <h2>COMP 5704: Parallel Algorithms and Applications in Data Science<br>
</h2>
      </td>
      <td height="27" width="10%">
      <p><br>
      </p>
      </td>
      <td height="27" width="45%">
      <p><b>School of Computer Science</b><br>
      <b>Carleton University, Ottawa, Canada</b></p>
      </td>
    </tr>
  </tbody>
</table>

<hr noshade="noshade">
<h2><font color="#005128">Project Title: </font><font><font color="#005128">Distributed method for training of Artificial Neural Networks</font></font></h2>

<h2><font><font color="#005128">Name: Seyed Omid Davoudi</font></font></h2>

<h2><font><font color="#005128">E-Mail: omiddavoudi at cmail.carleton.ca</font></font></h2>



<hr noshade="noshade">
<b><font color="#005128">Project Outline:</font></b> Machine Learning is currently experiencing a boom partly because of the recent advances in Deep Neural Networks. As time goes on, the datasets used for training these networks are getting larger and harder to handle. On the other side, the physical challenges of creating faster parallel processing hardware suitable for such tasks (GPUs, TPUs, etc.) has led to the growing interest in distributed methods for training neural networks.<br>
One of the interesting neural network architectures that has recently gained traction is that of the Generative Adversarial Network (GAN). GANs are used to create new and never-seen-before data points from observing a dataset, for example, generating pictures of cats after being trained on a dataset of cat images. These networks are notoriously hard to train and common distributed schemes for training neural nets have given poor results. This project aims to replacate the recent work of Hardy, Merrer and Sericola on a new architecture designed specifically to train GANs in a distributed manner.<br>
<p><b><font color="#005128">Startup Paper(s):</font></b> Click to go to page : <a href="https://ieeexplore-ieee-org.proxy.library.carleton.ca/document/8821025">MD-GAN: Multi-Discriminator Generative Adversarial Networks for Distributed Datasets</a></p>

<p><b><font color="#005128">Deliverables:</font></b></p>

<ul>

  <li>
    <div align="left"><a href="Literature_Review.pdf"><font color="#000000">Literature Review</font></a></div>
  </li>
  <li>
    <div align="left"><a href="Presentation_Outline.pdf"><font color="#000000">Presentation Outline</font></a></div>
  </li>
  <li>
    <div align="left"> <a href="Evaluating the MD-GAN architecture for distributed training of.pptx"><font color="#000000">Slide Presentation</font></a></div>
  </li>
  <li>
    <div align="left"><a href="Final_Paper.pdf"><font color="#000000">Final
Paper</font></a></div>
  </li>
  <li><a href="https://github.com/omiddavoodi/omiddavoodi.github.io/tree/master/comp5704/Code_and_Data"><font color="#000000">Code and Data</font></a></li>
</ul>

<p><b><font color="#005128">Relevant References:</font></b></p>
<ul>

  <li><a href="http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf">Goodfellow, Ian, et al. "Generative adversarial nets." Advances in neural information processing systems. 2014.</a></li>

  <li><a href="https://arxiv.org/pdf/1801.04406">Mescheder, Lars, Andreas Geiger, and Sebastian Nowozin. "Which training methods for GANs do actually converge?." arXiv preprint arXiv:1801.04406 (2018).</a></li>

  <li><a href="https://arxiv.org/pdf/1811.03850">Hardy, Corentin, Erwan Le Merrer, and Bruno Sericola. "MD-GAN: Multi-discriminator generative adversarial networks for distributed datasets." 2019 IEEE International Parallel and Distributed Processing Symposium (IPDPS). IEEE, 2019.</a></li>

  <li><a href="https://www.usenix.org/system/files/conference/osdi14/osdi14-paper-li_mu.pdf">Li, Mu, et al. "Scaling distributed machine learning with the parameter server." 11th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 14). 2014.</a></li>

  <li><a href="https://arxiv.org/pdf/1603.09035">Cano, Ignacio, et al. "Towards geo-distributed machine learning." arXiv preprint arXiv:1603.09035 (2016).</a></li>

  <li><a href="https://www.cs.iastate.edu/~honavar/noise-pac.pdf">Kearns, Michael. "Efficient noise-tolerant learning from statistical queries." Journal of the ACM (JACM) 45.6 (1998): 983-1006.</a></li>
  
  <li><a href="https://www.usenix.org/system/files/conference/nsdi17/nsdi17-hsieh.pdf">Hsieh, Kevin, et al. "Gaia: Geo-Distributed Machine Learning Approaching {LAN} Speeds." 14th {USENIX} Symposium on Networked Systems Design and Implementation ({NSDI} 17). 2017.</a></li>
  
  <li><a href="https://arxiv.org/pdf/1602.05629">McMahan, H. Brendan, et al. "Communication-efficient learning of deep networks from decentralized data." arXiv preprint arXiv:1602.05629 (2016).</a></li>
  
  <li><a href="https://arxiv.org/pdf/1412.6544">Goodfellow, Ian J., Oriol Vinyals, and Andrew M. Saxe. "Qualitatively characterizing neural network optimization problems." arXiv preprint arXiv:1412.6544 (2014).</a></li>
  
  <li><a href="http://papers.nips.cc/paper/4687-large-scale-distributed-deep-networks.pdf">Dean, Jeffrey, et al. "Large scale distributed deep networks." Advances in neural information processing systems. 2012.</a></li>
  
  <li><a href="https://papers.nips.cc/paper/6749-terngrad-ternary-gradients-to-reduce-communication-in-distributed-deep-learning.pdf">Wen, Wei, et al. "Terngrad: Ternary gradients to reduce communication in distributed deep learning." Advances in neural information processing systems. 2017.</a></li>
  
  <li><a href="https://link.springer.com/chapter/10.1007/978-3-030-02465-9_35">Vallecorsa, Sofia, et al. "Distributed Training of Generative Adversarial Networks for Fast Detector Simulation." International Conference on High Performance Computing. Springer, Cham, 2018.</a></li>
  
  <li><a href="https://arxiv.org/pdf/1802.05799">Sergeev, Alexander, and Mike Del Balso. "Horovod: fast and easy distributed deep learning in TensorFlow." arXiv preprint arXiv:1802.05799 (2018).</a></li>


</ul>

</body></html>
