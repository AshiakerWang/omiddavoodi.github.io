
% ===========================================================================
% Title:
% ---------------------------------------------------------------------------
% to create Type I fonts type "dvips -P cmz -t letter <filename>"
% ===========================================================================
\documentclass[11pt]{article}       %--- LATEX 2e base
\usepackage{latexsym}               %--- LATEX 2e base
%---------------- Wide format -----------------------------------------------
\textwidth=6in \textheight=9in \oddsidemargin=0.25in
\evensidemargin=0.25in \topmargin=-0.5in
%--------------- Slide --------------------------------------------------
\newenvironment{slide}[1]        {\section{#1} \begin{itemize}}%
                                 {\end{itemize}}
                                 % usage: \begin{slide}{---SLIDE TITLE---}
                                 %        \item ...
                                 %        \item ...
                                 %        \item ...
                                 %        \end{slide}
% ===========================================================================
\begin{document}
% ===========================================================================

% ############################################################################
% Title
% ############################################################################

\title{PRESENTATION OUTLINE: Determining the susceptibility of MD-GAN networks to nodes containing datasets with different class distributions}


% ############################################################################
% Author(s) (no blank lines !)
\author{
% ############################################################################
Omid Davoudi\\
School of Computer Science\\
Carleton University\\
Ottawa, Canada K1S 5B6\\
{\em omiddavoudi@cmail.carleton.ca}
% ############################################################################
} % end-authors
% ############################################################################

\maketitle

% ############################################################################
\begin{slide}{Distributed Machine Learning}
\item Very larger datasets
\item More computational power needed
\item Globalization of data
\item Regulations preventing data movement
\end{slide}

% ############################################################################
\begin{slide}{Distributed Machine Learning - Common Methods}
\item Parameter Server
\item Federated Learning
\item Gaia
\end{slide}

% ############################################################################
\begin{slide}{Generative Adversarial Networks (GANs)}
\item Generative models
\item Most commonly used for generating images
\item Consist of two neural networks, a Generator and a Discriminator
\item Training is tricky, convergence is hard to achieve
\end{slide}

% ############################################################################
\begin{slide}{MD-GAN}
\item GAN architecture designed for distributed training
\item Main idea is to share parameters for the generator, but have unsynchronized local discriminators
\item Assumes each node has a dataset with similar class distributions
\item Periodically switches discriminators between nodes to avoid overfitting on local datasets.
\end{slide}

% ############################################################################
\begin{slide}{MD-GAN - Potential vulnerability}
\item Differences in class distributions between different datasets
\item These differences always exist in real life
\item Theoretically, they can result in non-convergence
\item Switching discriminators between nodes with highly divergent class distributions could result in catastrophic forgetting
\end{slide}

% ############################################################################
\begin{slide}{MD-GAN - Testing the extent of vulnerability}
\item Proposed test setup: Cluster with at least 4 nodes
\item Distribute the data evenly among the nodes
\item Train until convergence
\item Create skewed datasets each with one class being overrepresented
\item Repeat the experiment with increasingly more skewed datasets
\end{slide}

% ############################################################################
\begin{slide}{MD-GAN - Vulnerability results}
\item Results go here: Convergence time (or lack of convergence) for each dataset
\end{slide}

% ############################################################################
\begin{slide}{MD-GAN - Potential avenue for training speed-up}
\item Training of the discriminator could happen while the generator is being synchronized
\item Training happens using the old generator
\item Better utilization of computational power
\item Could destabilize the training process or result in non-convergence
\item Test done with non-skewed datasets
\end{slide}

% ############################################################################
\begin{slide}{MD-GAN - Speed-up results}
\item Results go here: Convergence time (or lack of convergence) for using this method compared to not using it
\end{slide}

% ############################################################################
\begin{slide}{Conclusions}
\item MD-GAN is valid/invalid for real world/skewed datasets
\item MD-GAN speed-up did/did not give favourable results
\end{slide}

% ############################################################################
\begin{slide}{References}
\item References go here...
\end{slide}

% ############################################################################
% Bibliography
% ############################################################################
\bibliographystyle{plain}
\bibliography{my-bibliography}     %loads my-bibliography.bib

% ============================================================================
\end{document}
% ============================================================================
